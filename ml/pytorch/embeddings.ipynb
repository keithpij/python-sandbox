{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings: Encoding Linguistic (Lexical) Meaning (Semantics)\n",
    "### Contents\n",
    "Word Embeddings<br>\n",
    "Understanding the Samples<br>\n",
    "Exploring the Training Set<br>\n",
    "From Images to Numpy Arrays<br>\n",
    "From Numpy Arrays to Tensors<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to get the same result each time you create an embedding then set\n",
    "# the manual seed to the same value every time.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "word_to_token = {'hello': 0, 'world': 1}\n",
    "embeddings = nn.Embedding(2, 5)\n",
    "print(embeddings)\n",
    "\n",
    "lookup = torch.tensor(word_to_token['world'], dtype=torch.int64)\n",
    "world_embeddings = embeddings(lookup)\n",
    "print(world_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "EPOCHS = 1000\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text: str) -> list:\n",
    "    remove_breaks = raw_text.replace('<br />', ' ')\n",
    "    lower = remove_breaks.lower()\n",
    "    valid_characters = [c for c in lower if c not in punctuation]\n",
    "    cleaned = ''.join(valid_characters)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def create_n_grams(word_list: list, context_size: int) -> list:\n",
    "    # we should tokenize the input, but we will ignore that for now\n",
    "    # build a list of tuples.\n",
    "    # Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "    n_grams = [\n",
    "        (\n",
    "            [word_list[i - j - 1] for j in range(context_size)],\n",
    "            word_list[i]\n",
    "        )\n",
    "        for i in range(context_size, len(word_list))\n",
    "    ]\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def tokenize_n_grams(n_grams: list, words_to_tokens: dict) -> list:\n",
    "    n_grams_tokenized = [\n",
    "        (\n",
    "            [words_to_tokens[w] for w in context], \n",
    "            words_to_tokens[target]\n",
    "        )\n",
    "        for context, target in n_grams\n",
    "    ]\n",
    "    return n_grams_tokenized\n",
    "\n",
    "\n",
    "def get_word_from_token(token: int, words_to_tokens: dict) -> str:\n",
    "    word = [w for w in words_to_tokens if words_to_tokens[w] == token]\n",
    "    return word[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Shakespeare Sonnet 2\n",
    "training_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\"\n",
    "\n",
    "training_sentence = clean_text(training_sentence)\n",
    "training_words = training_sentence.split()\n",
    "\n",
    "# Turning our list of words into a set has the effect of eliminating duplicates.\n",
    "# This is a useful technique for getting a distinct list of words.\n",
    "vocab = set(training_words)\n",
    "\n",
    "# Now we can enumerate over the set and create a word to token mapping.\n",
    "# The index for each word within th set will become our token.\n",
    "words_to_tokens = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create n_grams (context and target) using the context size.\n",
    "n_grams = create_n_grams(training_words, CONTEXT_SIZE)\n",
    "\n",
    "# Tokenize the context and targets in the n_grams.\n",
    "n_grams_tokenized = tokenize_n_grams(n_grams, words_to_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 3, just so you can see what they look like.\n",
    "print('Clean sentence:', training_sentence, '\\n')\n",
    "print('n_grams:', n_grams[:3], '\\n')\n",
    "print('Vocabulary:', vocab, '\\n')\n",
    "print('Tokens:', words_to_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in n_grams_tokenized:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor(context, dtype=torch.int64)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        # target has to be a list for some reason.\n",
    "        loss = loss_function(log_probs, torch.tensor([target], dtype=torch.int64))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    losses.append(total_loss)\n",
    "\n",
    "# The loss should decrease with every iteration (epoch) over the training data.\n",
    "# When you have a large number of iterations over a small training set you are basically\n",
    "# memorizing your training set.\n",
    "# Print the first, last and every 100 in between.\n",
    "for epoch in range(0, EPOCHS, 100):\n",
    "    print(epoch, ':', losses[epoch])\n",
    "print(EPOCHS-1, ':', losses[EPOCHS-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[words_to_tokens['beauty']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tag for this context is 'old'.\n",
    "# Make sure all text here is lower case since the original\n",
    "# text was converted to lower case.\n",
    "context = ['art', 'thou'] \n",
    "\n",
    "context_indecies = torch.tensor([words_to_tokens[w] for w in context], dtype=torch.int64)\n",
    "log_probs = model(context_indecies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_probs)\n",
    "print(log_probs.shape)\n",
    "\n",
    "# The tensor of log probabilities is a two dimensional tensor (matrix)\n",
    "# because the model is expecting a batch of contextes.\n",
    "max_prob_index = torch.argmax(log_probs, dim=1)\n",
    "print(max_prob_index)\n",
    "\n",
    "print(log_probs[0, max_prob_index])\n",
    "predicted_word = get_word_from_token(max_prob_index, words_to_tokens)\n",
    "print('Predicted word:', predicted_word)\n",
    "\n",
    "top_predictions = torch.topk(log_probs, 5, dim=1, largest=True, sorted=True)\n",
    "print(top_predictions.indices[0])\n",
    "\n",
    "i = 0\n",
    "for index in top_predictions.indices[0]:\n",
    "    i += 1\n",
    "    predicted_word = get_word_from_token(index, words_to_tokens)\n",
    "    print(i, predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.embeddings.weight[words_to_tokens['cold']])\n",
    "print(model.embeddings.weight[words_to_tokens['winters']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([ 0.8539,  0.5130,  0.5397,  0.5655,  0.5058,  0.2225, -0.6855,  0.5636,\n",
    "        -1.5072, -1.6107], grad_fn=<SelectBackward0>)\n",
    "<br/>\n",
    "tensor([-0.2279,  0.8686, -1.4612, -0.9889, -0.2377,  1.8803,  0.3661, -0.4606,\n",
    "         0.3843, -0.9012], grad_fn=<SelectBackward0>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb30887f202a295f03f14bdc6c33a4c2546440b9bc6792bc5945ccf510842fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
