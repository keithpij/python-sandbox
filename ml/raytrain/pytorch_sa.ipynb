{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import preprocessor as pre\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'batch_size': 100,          # Batch size for each epoch\n",
    "    'dropout_prob': .5,         # Dropout probability\n",
    "    'embedding_dim': 400,       # Embedded dimension\n",
    "    'epochs': 5,                # Total number of epochs\n",
    "    'grad_clip': 10,             # Gradient Clip\n",
    "    'hidden_dim': 256,          # Hidden dimension\n",
    "    'lr': 0.001,                # Learning Rate\n",
    "    'n_layers': 2,              # Number of hidden layers in the LSTM\n",
    "    'output_dim': 1,            # Output dimension\n",
    "    'sequence_len': 200,        # Number of tokens (words) to put into each review.\n",
    "    'smoke_test_size': 0,       # Length of training set. 0 for all reviews.\n",
    "    'vocab_size': 7000          # Vocabulary size\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU is available.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test_size = config['smoke_test_size']\n",
    "data_file = os.path.join(os.getcwd(), 'aclImdb', 'IMDB Dataset.csv')\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()\n",
    "\n",
    "X, y = df['review'].values, df['sentiment'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=45000, stratify=y, random_state=42)\n",
    "if smoke_test_size:\n",
    "    X_train = X_train[0:smoke_test_size]\n",
    "    y_train = y_train[0:smoke_test_size]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=40000, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pre.preprocess_train_valid_data(config)\n",
    "X_test, y_test = pre.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape of train data is {X_train.shape}')\n",
    "print(f'shape of validation data is {X_valid.shape}')\n",
    "print(f'shape of test data is {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']), y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r'\\s+', '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r'\\d', '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def pad(X, sequence_len):\n",
    "    features = np.zeros((len(X), sequence_len),dtype=int)\n",
    "    for ii, review in enumerate(X):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:sequence_len]\n",
    "    return features\n",
    "\n",
    "def create_tokens(X_train, config):\n",
    "    vocab_size = config['vocab_size']\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    for entry in X_train:\n",
    "        for word in entry.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "\n",
    "    count_by_word = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    count_by_word_sorted = sorted(count_by_word, key=count_by_word.get, reverse=True)[:vocab_size-1]\n",
    "    # creating a dict\n",
    "    word_to_int_mapping = {w:i+1 for i,w in enumerate(count_by_word_sorted)}\n",
    "    return word_to_int_mapping\n",
    "\n",
    "def tokenize(X, y, mapping, config):\n",
    "    sequence_len = config['sequence_len']\n",
    "    new_X = []\n",
    "    for entry in X:\n",
    "        new_X.append([mapping[preprocess_string(word)] for word in entry.lower().split() \n",
    "                                    if preprocess_string(word) in mapping.keys()])\n",
    "            \n",
    "    new_X = pad(new_X, sequence_len)\n",
    "    new_y = [1 if label =='positive' else 0 for label in y]  \n",
    "    \n",
    "    return np.array(new_X), np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int_mapping = create_tokens(X_train, config)\n",
    "X_train, y_train = tokenize(X_train, y_train, word_to_int_mapping, config)\n",
    "X_valid, y_valid = tokenize(X_valid, y_valid, word_to_int_mapping, config)\n",
    "X_test, y_test = tokenize(X_test, y_test, word_to_int_mapping, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    '''\n",
    "    An LSTM is a type of RNN network that can be used to perform Sentiment analysis.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, vocab_size, output_dim, embedding_dim, hidden_dim, n_layers, batch_size, dropout_prob):\n",
    "        '''\n",
    "        Initialize the model and set up the layers.\n",
    "        '''\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "        #self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Linear layer\n",
    "        self.fcl = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Sigmoid layer\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fcl(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=None):\n",
    "        ''' \n",
    "        Initializes hidden state\n",
    "        Creates two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        initialized to zero, for hidden state and cell state of LSTM.\n",
    "\n",
    "        Note: The batch_size needs to be 1 for predictions.\n",
    "        '''\n",
    "        if not batch_size:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        h0 = torch.zeros((self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "n_layers = config['n_layers']\n",
    "vocab_size = config['vocab_size']\n",
    "embedding_dim = config['embedding_dim']\n",
    "output_dim = config['output_dim']\n",
    "hidden_dim = config['hidden_dim']\n",
    "dropout_prob = config['dropout_prob']\n",
    "\n",
    "model = SentimentLSTM(vocab_size, output_dim, embedding_dim, hidden_dim, n_layers, batch_size, dropout_prob)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr = config['lr']\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Accuracy function\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_clip = config['grad_clip']\n",
    "epochs = config['epochs']\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "training_loss_by_epoch, valid_loss_by_epoch = [],[]\n",
    "training_acc_by_epoch, valid_acc_by_epoch = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        # If system has a GPU this will move the data to the GPU's memory.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        \n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(valid_loader.dataset)\n",
    "\n",
    "    training_loss_by_epoch.append(epoch_train_loss)\n",
    "    valid_loss_by_epoch.append(epoch_val_loss)\n",
    "    training_acc_by_epoch.append(epoch_train_acc)\n",
    "    valid_acc_by_epoch.append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'Training Loss: {epoch_train_loss} Validation Loss: {epoch_val_loss}')\n",
    "    print(f'Training Accuracy: {epoch_train_acc*100} Validation Accuracy: {epoch_val_acc*100}')\n",
    "\n",
    "    print(25*'==')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_loss_by_epoch, label='Training Loss')\n",
    "plt.plot(valid_loss_by_epoch, label='Validation Loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_acc_by_epoch, label='Training Accuracy')\n",
    "plt.plot(valid_acc_by_epoch, label='Validation Accuracy')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc919a8c9f3b0b92e1d085faab71fe86a296093fb313f99140bffe2c1d0fe07d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
